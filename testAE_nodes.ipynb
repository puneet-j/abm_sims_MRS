{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn\n",
    "# import torch_geometric\n",
    "from torch_geometric.nn import SAGEConv\n",
    "# from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import pdb\n",
    "from torch_geometric.loader import DataLoader\n",
    "# import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.utils import to_dense_adj, subgraph, k_hop_subgraph\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.combine_nx_to_dataloader import GraphDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=40, encoded_dim=3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # Adjusted to include two additional layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 30),  # Reducing from input dimension to 30\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(30, 25),  # Additional layer reducing to 25 dimensions\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(25, 20),  # Additional layer reducing to 20 dimensions\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20, encoded_dim)  # Final layer reducing to the encoded dimensions\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        # Adjusted to include two additional layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoded_dim, 20),  # Expanding from encoded dimensions to 20\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20, 25),  # Additional layer expanding to 25 dimensions\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(25, 30),  # Additional layer expanding to 30 dimensions\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(30, input_dim),  # Final layer to reconstruct the original input size\n",
    "            nn.Sigmoid()  # Using sigmoid to ensure the output is between 0 and 1\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "node_list = []\n",
    "folder_graph = './graphs/graphsage_graph/single_graphs/'\n",
    "metadata_file = folder_graph + 'metadata.csv'\n",
    "succTimeFiles = folder_graph + 'graphMetadata.csv'\n",
    "\n",
    "files = os.listdir(folder_graph)\n",
    "files = [file for file in files if file.endswith('.pickle')]\n",
    "for file in files:\n",
    "    fil =  open(folder_graph+file, 'rb')\n",
    "    G = pickle.load(fil)\n",
    "    fil.close()\n",
    "    # print(G.nodes[0])\n",
    "    for node in G.nodes(data=True):\n",
    "        # print(node[1]['x'])\n",
    "        # break\n",
    "        node_list.append(node[1]['x'])\n",
    "    # break\n",
    "\n",
    "# dataset = GraphDataset(graph_list)\n",
    "# dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Save the graph list for later use, similar to the previous example\n",
    "torch.save(node_list, 'nodelist.pth')\n",
    "\n",
    "# # Example of iterating over the DataLoader in a training loop\n",
    "# for node_features, edge_index, edge_features in dataloader:\n",
    "#     # Use these in your GAE model\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "folder_graph = './graphs/graphsage_graph/single_graphs/'\n",
    "fname = 'nodelist.pth'\n",
    "nodelist = torch.load(fname)\n",
    "data_nodes = torch.tensor(nodelist, dtype=torch.float32)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the item at the given index\n",
    "        return self.data[index]\n",
    "dataset = CustomDataset(data_nodes)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.6416\n",
      "Epoch 2, Loss: 1.6414\n",
      "Epoch 3, Loss: 1.6414\n",
      "Epoch 4, Loss: 1.6415\n",
      "Epoch 5, Loss: 1.6414\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# Initialize the autoencoder\n",
    "generated_embeds = []\n",
    "losses = []\n",
    "for ed in range(3,10):\n",
    "    autoencoder = Autoencoder(encoded_dim=ed)\n",
    "\n",
    "    # Example of a loss function and optimizer\n",
    "    criterion = nn.MSELoss() # Mean Squared Error Loss\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.1) # Adam optimizer\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # d = [a for a in dataloader]\n",
    "    # print(d)\n",
    "    loss_vec = []\n",
    "    embed_vec = []\n",
    "    for epoch in range(1, 50):\n",
    "        total_loss = 0.0\n",
    "        for input_vector in dataloader:  # Assuming the dataset returns a tuple where the first element is the input\n",
    "            optimizer.zero_grad()\n",
    "            output, embed = autoencoder(input_vector)\n",
    "            # print(input_vector[0], '\\n', output)\n",
    "            loss = criterion(output, input_vector)  # Assuming target is the same as input\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Average loss for this epoch\n",
    "        epoch_loss = total_loss / len(dataloader)\n",
    "        embed_vec.append(embed)\n",
    "        loss_vec.append(epoch_loss)\n",
    "        # Step the scheduler based on the epoch loss\n",
    "        scheduler.step(epoch_loss)\n",
    "        gc.collect()\n",
    "        # if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {epoch_loss:.4f}')\n",
    "    losses.append(loss_vec)\n",
    "    generated_embeds.append(embed_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
